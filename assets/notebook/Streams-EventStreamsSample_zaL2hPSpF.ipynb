{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# IBM Streams Event Streams sample application\n\nThis sample demonstrates how to create a Streams Python application that ingests data into the [IBM Event Streams](https://cloud.ibm.com/catalog?search=Event%20Streams) service, and consumes the data from Event Streams. The IBM Event Streams service is a fully managed Kafka Service within the IBM cloud.\n\nIn this notebook, you'll see examples of how to :\n 1. [Setup your data connections](#setup)\n 2. [Create the application](#create)\n 3. [Submit the application](#launch)\n 4. [Connect to the running application to view data](#view)\n 5. [Stop the application](#cancel)\n\n# Overview\n\n**About the sample**\n\nThis application creates artificial sensor data and writes them into a topic in the IBM Event Streams instance, subscribes to the same topic and filters out the data of one sensor.\n\n**How it works**\n\nThe Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to retrieve the results.\n\n<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n\n\n### Documentation\n\n- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n\n\n\n<a name=\"setup\"></a>\n# 1. Setup\n### 1.1 Add credentials for the IBM Streams service\n\nIn order to submit a Streams application you need to provide the name of the Streams instance.\n\n1. From the navigation menu, click **My instances**.\n2. Click the **Provisioned Instances** tab.\n3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name."}, {"metadata": {}, "cell_type": "code", "source": "from icpd_core import icpd_util\nstreams_instance_name = \"cp4d-streams-instance\" ## Change this to Streams instance\ncfg=icpd_util.get_service_instance_details(name=streams_instance_name)", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Optional: Upgrade the `streamsx.eventstreams` Python package\n\nUncomment and run the cell below if you want to upgrade to the latest version of the `streamsx.eventstreams` package.\n"}, {"metadata": {}, "cell_type": "code", "source": "#!pip install --user --upgrade streamsx.eventstreams", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The python packages will be installed in the top of user path.<br/>\nIf you have problem to get the latest version of python packages you can set the order of python packages manually to user path.<br/>\nyou can find the user path with this command:<br/>\n`\nimport sys\nfor e in sys.path:\n    print(e)\n`"}, {"metadata": {}, "cell_type": "code", "source": "#import sys\n#sys.path.insert(0, '/home/wsuser/.local/lib/python3.6/site-packages')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import streamsx.eventstreams as eventstreams\nimport streamsx.topology.context\nprint(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\nprint(\"INFO: streamsx.eventstreams package version: \" + eventstreams.__version__)", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "INFO: streamsx package version: 1.13.14\nINFO: streamsx.eventstreams package version: 1.3.1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3 Configure the connection to the IBM Event Streams service\n\nTo connect with the Event Streams cloud service, we need service credentials, and at least one topic within the service instance.\n\nTo create the credentials and a topic, do the following steps:\n\n1. Create an Event Streams service instance on IBM cloud.\n\n   You need to have an IBM account to be able to do this.\n   \n   https://cloud.ibm.com/catalog?search=Event%20Streams\n   <br>\n   \n1. Under *Topics*, create one topic. You can use the default values for all settings. The topic name will be used later in the notebook.\n1. Under *Service credentials*, create new credentials. You can leave all settings at their defaults.\n1. View the created credentials, and copy them to the clipboard\n1. Paste the credential into the `Your Event Streams credentials:` prompt in the next cell.\n"}, {"metadata": {}, "cell_type": "code", "source": "import getpass\neventstreams_credentials_json = getpass.getpass('Your Event Streams credentials:')", "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Your Event Streams credentials:\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Create an application configuration in the IBM Streams service for the Event Streams service credentials.\nThis is the safest way to avoid the credentials being exposed."}, {"metadata": {}, "cell_type": "code", "source": "# create an application configuration\nfrom streamsx.rest import Instance\n\ncfg[streamsx.topology.context.ConfigParams.SSL_VERIFY] = False\ninstance = Instance.of_service(cfg)\napp_config_name = eventstreams.configure_connection(instance,\n                                                    name='eventstreams',\n                                                    credentials=eventstreams_credentials_json)\nprint(\"INFO: Name of your application configuration: \" + app_config_name)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "/user-home/_global_/python-3/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n", "name": "stderr"}, {"output_type": "stream", "text": "create application configuration: eventstreams\nINFO: Name of your application configuration: eventstreams\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "In the Event Streams service, create the *topic* where you want to publish the data. You can use the default settings for partitions and retention hours. Enter the topic name when you run the next cell.\n"}, {"metadata": {}, "cell_type": "code", "source": "topic = 'cp4dtest0310'# Enter the topic name here", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n<a id=\"create\"></a>\n# 2. Create the application\n\nThis application is going to ingest readings from simulated sensors into a topic in the Event Streams service. Another part of the application subscribes to the topic and filters out one sensor of interest.  \n\nAll Streams applications start with  a `Topology` object, so start by creating one:\n"}, {"metadata": {}, "cell_type": "code", "source": "from streamsx.topology.topology import Topology\n\ntopo = Topology(name=\"EventStreamsSample\")", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.1 Define sources\nYour application needs some data to analyze, so the first step is to define a data source that produces the data being processed. \n\nNext, use the data source to create a `Stream` object. A `Stream` is a potentially infinite sequence of tuples containing the data to be analyzed.\n\nIn this example, we use JSON objects, which are Python dicts. Other supported formats include Strings, structured tuples, and more. [See the doc for all supported formats](http://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#stream)."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.1 Define a source class\n\nDefine a *callable* class that will produce the data to be analyzed.\n\nThis example class produces readings from sensors."}, {"metadata": {}, "cell_type": "code", "source": "import random \nimport time\nfrom datetime import datetime, timedelta\n\n# define a callable source \nclass SensorReadingsSource(object):\n    def __call__(self):\n        # This is just an example of using generated data, \n        # Here you could connect to db\n        # generate data\n        # connect to data set\n        # open file\n        \n        while True:\n            time.sleep(0.005)\n            sensor_id = random.randint(1,100)\n            reading = {}\n            reading[\"sensor_id\"] = \"sensor_\" + str(sensor_id)\n            reading[\"value\"] =  random.random() * 3000\n            reading[\"ts\"] = int((datetime.now().timestamp()))\n            yield reading", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2  Create the `Stream `\n\nCreate a `Stream` with `CommonSchema.Json` schema called  `Readings` that will contain the simulated data that `SensorReadingsSource` produces:"}, {"metadata": {}, "cell_type": "code", "source": "# create a stream from the data using Topology.source\nreadings = topo.source(SensorReadingsSource(), name=\"Readings\").as_json()", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.2 Publish the tuples in the Event Streams service\n\nNow publish the data of the `readings` stream to the topic you have configured in the `topic` variable.\n"}, {"metadata": {}, "cell_type": "code", "source": "eventstreams.publish(readings,\n                     topic,\n                     credentials=app_config_name,\n                     name=\"EventStrPublish\")", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "<streamsx.topology.topology.Sink at 0x7fc994618ac8>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "**Summary:**\n\nBy now, you have defined a streaming application that generates simulated data and publishes the data in a topic within an Eventstreams service. You could submit the application now, so that any other application could consume the data from the Eventstreams service.\n\nIn the next steps, you extend the `topo` topology by a consumer that consumes and analyzes the data."}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.3 Subscribe to the Eventstreams topic and consume the data\n\nWhen you subscribe to the topic, you create a new data source, that connects to the Eventstreams service. The stream of data shall have the  `Json` schema."}, {"metadata": {}, "cell_type": "code", "source": "from streamsx.topology.schema import CommonSchema\n# create a new Json stream in the topology\nsensordata = eventstreams.subscribe(topo,\n                                    topic,\n                                    CommonSchema.Json,\n                                    credentials=app_config_name,\n                                    name=\"EventStrSubscribe\")", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.4 Analyze the data\n\nUse a variety of methods in the `Stream` class to analyze your in-flight data, including applying machine learning models.\n\nSee the [common operations section](https://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-4/) of the developer guide and the [documentation on the Stream class](https://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#streamsx.topology.topology.Stream) for more details.\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2.1 Filter data from the  `Stream`  \n\nUse `Stream.filter()` to pass through only data that match a certain condition."}, {"metadata": {}, "cell_type": "code", "source": "# in this example, pass through only sensor data from sensor with ID \"sensor_3\"\n\nsensordata_id3 = sensordata.filter(lambda x: x[\"sensor_id\"] == \"sensor_3\",\n                                   name=\"SensorsId3\")\n\n# you could create another stream of the other sensors:\n#sensordata_other = sensordata.filter(lambda x: x[\"sensor_id\"] != \"sensor_3\", name=\"OtherSensors\")\n", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2.3 Create a `View` to preview the tuples on the `Stream` \n\n\nA `View` is a connection to a `Stream` that becomes activated when the application is running. We examine the data from within the notebook in section 4, below.\n"}, {"metadata": {}, "cell_type": "code", "source": "sensor3_view = sensordata_id3.view(name=\"Sensor3\",\n                                   description=\"Sample of sensor with ID sensor_3\")", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2.4 Define output\n\nThe `sensordata_id3` stream is our final result. We will use `Stream.publish()` to make this stream available to other Streams applications. \n\nIf you want to send the stream to another database or system, you would use a sink function (similar to the source function) and invoke it using `Stream.for_each`.\n\nYou can also the functions of other Python packages to send the stream to other systems, for example the eventstore."}, {"metadata": {}, "cell_type": "code", "source": "import json\n# publish results as JSON\nsensordata_id3.publish(topic=\"SensorData\",\n                       schema=json,\n                       name=\"PublishSensors\")\n\n# other options include:\n# invoke another sink function:\n#sensordata_id3.for_each(func=send_to_db)\n", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "<streamsx.topology.topology.Sink at 0x7fc99462c278>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"launch\"></a>\n\n# 3. Submit the application\nA running Streams application is called a *job*. This next cell submits the application for execution and prints the resulting job id."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "from streamsx.topology import context\n\n# disable SSL certificate verification if necessary\ncfg[context.ConfigParams.SSL_VERIFY] = False\n# submit the topology 'topo'\nsubmission_result = context.submit(\"DISTRIBUTED\", topo, config=cfg)\n\n# the submission_result object contains information about the running application, or job\nif submission_result.job:\n    streams_job = submission_result.job\n    print(\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3801031258ab49998a53f48930856a2e"}}, "metadata": {}}, {"output_type": "stream", "text": "Insecure host connections enabled.\nInsecure host connections enabled.\nInsecure host connections enabled.\n/user-home/_global_/python-3/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n", "name": "stderr"}, {"output_type": "stream", "text": "JobId:  58 \nJob name:  StreamsTutorialandTestbed::EventStreamsSample_58\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"view\"></a>\n\n# 4. Use a `View` to access data from the job\nNow that the job is started, use the `View` object you created in step 2.3 to start retrieving data from a `Stream`."}, {"metadata": {}, "cell_type": "code", "source": "# connect to the view and display the data\nqueue = sensor3_view.start_data_fetch()\ntry:\n    for val in range(10):\n        print(queue.get())    \nfinally:\n    sensor3_view.stop_data_fetch()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.1 Display the results in real time\nCalling `View.display()` from the notebook displays the results of the view in a table that is updated in real-time."}, {"metadata": {}, "cell_type": "code", "source": "# display the results for 30 seconds\nsensor3_view.display(duration=30)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n## 4.2 See job status \n\nYou can view job status and logs by going to **My Instances** > **Jobs**. Find your job based on the id printed above.\nRetrieve job logs using the \"Download logs\" action from the job's context menu.\n\nTo view other information about the job such as detailed metrics, access the graph. Go to **My Instances** > **Jobs**. Select \"View graph\" action for the running job.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"cancel\"></a>\n\n# 5. Cancel the job"}, {"metadata": {}, "cell_type": "markdown", "source": "This cell generates a widget you can use to cancel the job."}, {"metadata": {}, "cell_type": "code", "source": "# cancel the job in the IBM Streams service\nsubmission_result.cancel_job_button()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can also interact with the job through the [Job](https://streamsxtopology.readthedocs.io/en/stable/streamsx.rest_primitives.html#streamsx.rest_primitives.Job) object returned from `submission_result.job`\n\nFor example, use `job.cancel()` to cancel the running job directly."}, {"metadata": {}, "cell_type": "markdown", "source": "# Summary\n\nWe started with a `Stream` called `readings`, which contained the data that we published in the Event Streams service. Next, we created a new Stream `sensordata` by subscribing to the topic in the EventStreams Service, filtered out one sensor of interest, and `published` the filtered stream for other applications running within our Streams instance to access.\n\nAfter submitting the application to the IBM Streams service, we connected to the `sensor3_view` view to see the data of sensor 3 within the notebook.\n\nYou may have noticed that the application consists of two independent parts: One part generates the data and publishes the them to the Event Streams cloud service. The other part consumes from Event Streams, filters, and publishes the stream within the IBM Streams instance. These two parts can also be declared by using different topologies, and can be submitted as separate jobs."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}