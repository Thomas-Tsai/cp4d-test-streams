{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# IBM Streams basic Kafka sample application\n\nThis sample demonstrates how to create a Streams Python application that connects to a Kafka cluster. The focus of this notebokk is on creating a connection with a Kafka cluster.\n\nA Kafka cluster is typically setup and configured by an administrator, but it is also possible that you setup a single Kafka broker on a virtual or physical machine by yourself following the instructions on https://kafka.apache.org/quickstart. In this case you know the details, how to connect, and what topics can be used. Otherwise the administrator of the Kafka cluster must provide the required information.\n\nIn this notebook, you'll see examples of how to:\n 1. [Setup your data connections](#setup)\n 1. [Create the consumer application](#create_1)\n 1. [Create a simple producer application](#create_2)\n 1. [Submit the applications](#launch)\n 1. [Connect to the running consumer application to view data](#view)\n 1. [Stop the applications](#cancel)\n\n# Overview\n\n**About the sample**\n\nThe main goal of the sample is to show how to connect to a Kafka cluster and how to create a Kafka consumer group. A consumer group is mostly used to consume partitioned topics with multiple consumers sharing the partitions. The messages are typically distributed to the partitions by using a *key*.\n\nConsuming a single-partition topic with more than one consumer has no advantage as failed consumers are restarted by Streams nearly as quickly as a failover of the single partition to another consumer would take.\n\nFor completion of this sample there is also a data generator, which publishes data to the topic.\n\n**How it works**\n\nThe Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to retrieve the results.\n\n<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n\n\n### Documentation\n\n- [Kafka consumer groups](https://kafka.apache.org/documentation/#intro_consumers)\n- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n- [streamsx.kafka Python package](https://streamsxkafka.readthedocs.io/)\n\n\n\n<a name=\"setup\"></a>\n# 1. Setup\n### 1.1 Add credentials for the IBM Streams service\n\nIn order to submit a Streams application you need to provide the name of the Streams instance.\n\n1. From the navigation menu, click **My instances**.\n2. Click the **Provisioned Instances** tab.\n3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name."}, {"metadata": {}, "cell_type": "code", "source": "from icpd_core import icpd_util\nstreams_instance_name = \"cp4d-streams-instance\" ## Change this to Streams instance\ncfg=icpd_util.get_service_instance_details(name=streams_instance_name)", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Optional: Upgrade the `streamsx.kafka` Python package\n\nUncomment and run the cell below to upgrade to the latest version of the `streamsx.kafka` package.\n"}, {"metadata": {}, "cell_type": "code", "source": "#!pip install --user --upgrade streamsx.kafka", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The python packages will be installed in the top of user path.<br/>\nIf you have problem to get the latest version of python packages you can set the order of python packages manually to user path.<br/>\nyou can find the user path with this command:<br/>\n`\nimport sys\nfor e in sys.path:\n    print(e)\n`"}, {"metadata": {}, "cell_type": "code", "source": "#import sys\n#sys.path.insert(0, '/home/wsuser/.local/lib/python3.6/site-packages')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport streamsx.kafka as kafka\nimport streamsx.topology.context as context\nprint (\"INFO: streamsx package version: \" + context.__version__)\nprint (\"INFO: streamsx.kafka package version: \" + kafka.__version__)", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "INFO: streamsx package version: 1.13.14\nINFO: streamsx.kafka package version: 1.6.1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3 Configure the connection to the Kafka cluster\n\nKafka consumers and producers are configured by properties. They are described in the Kafka documentation in separate sections for [producer configs](https://kafka.apache.org/22/documentation.html#producerconfigs) and [consumer configs](https://kafka.apache.org/22/documentation.html#consumerconfigs). In Python, you will be using a `dict` variable for the properties.\n\nThe operators of the underlying SPL toolkit set defaults for some properties. You can review these operator provided defaults in the [toolkit documentation](https://ibmstreams.github.io/streamsx.kafka/doc/spldoc/html/) under **Operators**.\n\nThe most important setting is the `bootstrap.servers` configuration, which is required for both consumers and producers. This config has the form\n```\nhost1:port1,host2:port2,....\n```\nSince these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), this list need not contain the full set of servers (you may want more than one, though, in case a server is down).\n\nThe required properties for consumers and producers depend on the configuration of the Kafka cluster.\n- Connection over TLS, yes or no?\n- If TLS is in place, is the server certificate trusted, e.g. signed by a public root CA? If not, you will need the server certificate to configure a truststore.\n- Is client or user authentication used?\n- If yes, what authentication mechanism is configured?\n  Dependent on the authentication mechanism you will need additional secrets, for example username and password, or client certificate and private key.\n\nFor example, [AMQ Streams](https://access.redhat.com/products/red-hat-amq#streams), a Kafka cluster for the Openshift container platform, supports encryption with TLS, and authentication using TLS certificates or SCRAM-SHA-512.\n\nBefore you begin, you must gather the required connection details. Once you have all information, it is quite comfortable to create the properties for consumers and producers. For the notebook, create a partitioned topic, for example with three partitions or ask the administrator to do this for you.\n\n\n### 1.3.1 Handling of certificates and keys in the notebook\n\nWhen you need certificates or keys, you must provide them in PEM format. The PEM format is a text format with base64 coded data enclosed in BEGIN and END anchors. You can add certificates or keys directly to the Python code, for example\n\n    ca_server_cert = \"\"\"\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n    \"\"\"\n\nor you can upload certificate and key files as *Data Assets* to your project and use the file names of the local files.\n\n    ca_server_cert = '/project_data/data_asset/<your dataset name>'\n\nIn the Kafka broker, create a partitioned topic, for example with three partitions.\n\n\n<a id=\"create_1\"></a>\n# 2. Create the consumer application\n\nThis application subscribes to a Kafka topic by using a consumer group (multiple consumers that share a group identifier).\n\nWe assume that the messages we fetch from the topic, are JSON formatted with the content like\n```\n{\"sensor_id\": \"sensor_4545\", \"value\": 3567.87, \"ts\": 1559029421}\n```\n\nAll Streams applications start with a Topology object, so start by creating one:"}, {"metadata": {}, "cell_type": "code", "source": "from streamsx.topology.topology import Topology\nfrom streamsx.topology.context import submit, ContextTypes\nfrom streamsx.topology.topology import Routing\nfrom streamsx.topology.schema import StreamSchema\nfrom streamsx.kafka.schema import Schema\nfrom streamsx.kafka import AuthMethod\n\nconsumer_topology = Topology(name='KafkaBasicSample-Consumer')", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.1 Create the consumer properties from your connection details\n\nUse the helper function [create_connection_properties(...)](https://streamsxkafka.readthedocs.io/en/latest/#streamsx.kafka.create_connection_properties) to create the properties.\n\nDependent on the Kafka cluster configuration you may need\n- A trusted server CA certificate\n- Information about the authentication method. The function supports following authentication methods:\n  - No authentication\n  - SASL/PLAIN - you need a username and a password\n  - SASL/SCRAM-SHA-512 - you need a username and a password\n  - TLS - you need a client certificate and the private key of the certificate.\n\nYou always need a topic name that can be accessed. Enter at least the bootstrap servers and the topic name into the below cell."}, {"metadata": {}, "cell_type": "code", "source": "topic = \"test\"  ## change this to an existing topic, it should have multiple partitions\nkafka_group_id = \"\"  ## change the consumer group identifier if required\nbootstrap_servers = \"ka.libthomas.org:9092\"    ## change the bootstrap server(s) here\n\n# this template connects to an unsecured (no TLS) cluster without authentication \nconnect_tls = False     # set True when Kafka must be connected with TLS\nca_server_cert = None   # use PEM or filename if required, see section 1.3.1\nauth = AuthMethod.NONE  # chose one of NONE, TLS, PLAIN, SCRAM_SHA_512\nclient_cert = None      # use PEM or filename if auth is TLS\nclient_priv_key = None  # use PEM or filename if auth is TLS\nusername = None         # required for PLAIN and SCRAM_SHA_512\npassword = None         # required for PLAIN and SCRAM_SHA_512\n\nconsumer_configs = kafka.create_connection_properties(\n    bootstrap_servers=bootstrap_servers,\n    use_TLS=connect_tls,      \n    enable_hostname_verification=True,\n    cluster_ca_cert=ca_server_cert,\n    authentication=auth,\n    client_cert=client_cert,\n    client_private_key=client_priv_key,\n    username=username,\n    password=password,\n    topology=consumer_topology)\n\n# print the consumer configs for reference. Note, that they can contain sensitive data\nprint()\nfor key, value in consumer_configs.items():\n    print(key + \"=\" + value)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "\nbootstrap.servers=ka.libthomas.org:9092\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<div class=\"alert alert-block alert-warning\">\n    <b>Warning:</b>\nWhen a certificate or private key is used to create properties, the topology parameter must not be <tt>None</tt>. In this case, the function <tt>create_connection_properties</tt> creates a keystore and/or a truststore file, which are attached as file dependencies to the topology, whereas the filenames go into the created properties.\n    \nThese properties can therefore not be used together with a different topology.\n</div>\n\nThe `group.id` config is not required here. The group identifier is specified on Python API level later. However, when you need other special [Kafka consumer configs](https://kafka.apache.org/22/documentation.html#consumerconfigs), you should add them to the `consumer_configs` dict variable here.\n\n## 2.2 Create the consumer group\n\nFrom the Kafka broker we fetch keyed messages, where the message type is a string. In addition to it we want to fetch the message meta data, like partition number, message timestamp, and other.\n\nThat's why we specify `Schema.StringMessageMeta` as the schema for the created Stream in the [kafka.subscribe](https://streamsxkafka.readthedocs.io/en/latest/index.html#streamsx.kafka.subscribe) function.\n\nThis schema is a structured schema that defines following attributes:\n\n- message(str) - the message content\n- key(str) - the key for partitioning\n- topic(str) - the Kafka topic\n- partition(int) - the topic partition number (32 bit)\n- offset(int) - the offset of the message within the topic partition (64 bit)\n- messageTimestamp(int) - the message timestamp in milliseconds since epoch (64 bit)\n\nCreate the stream `received` by subscribing to the Kafka topic, parallelize the source with `set_parallel` and combine the parallel streams with `end_parallel`. The result is a stream created by a consumer group with three consumers.\n"}, {"metadata": {}, "cell_type": "code", "source": "consumerSchema = Schema.StringMessageMeta\nreceived = kafka.subscribe(\n    consumer_topology,\n    topic=topic,\n    schema=consumerSchema,\n    group=kafka_group_id,  # when not specified it is the job name, concatenated with the topic\n    kafka_properties=consumer_configs,\n    name=\"SensorSubscribe\"\n    ).set_parallel(3).end_parallel()\n", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "properties file /tmp/wsuser/consumer-SensorSubscribe.test.properties generated.\nProperties file etc/consumer-SensorSubscribe.test.properties added to the topology KafkaBasicSampleConsumer\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"create_view\"></a>\n## 2.3 Create a `View` to preview the received tuples on the `Stream` \n\nA `View` is a connection to a `Stream` that becomes activated when the application is running. We examine the data from within the notebook in [section 5](#view), below.\n"}, {"metadata": {}, "cell_type": "code", "source": "streamView = received.as_string().view(name=\"ReceivedSensorData\",\n                                       description=\"received sensor data\")", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.5 Define output\n\nThe `received` stream is our final result. We will use `Stream.publish()` to make this stream available to other Streams applications.\n\nIf you want to send the stream to another database or system, you would use a sink function and invoke it using `Stream.for_each`. You can also use the functions of other Python packages to send the stream to other systems, for example the eventstore."}, {"metadata": {}, "cell_type": "code", "source": "# publish results as Strings\nreceived.publish(topic=\"SensorData\",\n                 schema=str,\n                 name=\"PublishSensors\")\n\n# other options include:\n# invoke another sink function:\n# received.for_each(func=send_to_db)\n# received.print()", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "<streamsx.topology.topology.Sink at 0x7fb72aa086a0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create_2\"></a>\n# 3. Create a simple producer application\n\nTo make the consumer application work, we need to publish some data to the topic. Therefore we create another application that publishes data to the Kafka cluster.\n\n## 3.1 Define a data generator for the messages and a source stream"}, {"metadata": {}, "cell_type": "code", "source": "import random\nimport time\nimport json\nfrom datetime import datetime\n\n\n# define a callable source for data that we publich in Kafka\n     \nclass SensorReadingsSource(object):\n    def __call__(self):\n        # this is just an example of using generated data, here you could\n        # - connect to db\n        # - generate data\n        # - connect to data set\n        # - open a file\n        i = 0\n        while(i < 500000):\n            time.sleep(0.001)\n            i = i + 1\n            sensor_id = random.randint(1, 100)\n            reading = {}\n            reading[\"sensor_id\"] = \"sensor_\" + str(sensor_id)\n            reading[\"value\"] = random.random() * 3000\n            reading[\"ts\"] = int(datetime.now().timestamp())\n            yield reading\n\nproducer_topology = Topology(name='KafkaBasicSample-Producer')\n\n# create the data and map them to the attributes 'message' and 'key' of the\n# 'Schema.StringMessage' schema for Kafka, so that we have messages with keys\nsensorStream = producer_topology.source(\n    SensorReadingsSource(),\n    \"RawDataSource\"\n    ).map(\n        func=lambda reading: {'message': json.dumps(reading),\n                              'key': reading['sensor_id']},\n        name=\"ToKeyedMessage\",\n        schema=Schema.StringMessage)\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.2 Publish the data to the Kafka topic\n\nFor advanced producer configurations please review the [producer configs section](https://kafka.apache.org/21/documentation.html#producerconfigs) of the Kafka documentation. Here we setup only the `bootstrap.servers` as used for the consumers. For a Kafka basic install out of the box this is sufficient. When you have configured authentication or other security options for the consumer, you must configure the same options also for the producer.\n\n### 3.2.1 Create the producer properties from your connection details\n\nWe will be using the helper function [create_connection_properties(...)](https://streamsxkafka.readthedocs.io/en/latest/#streamsx.kafka.create_connection_properties), but will use the `producer_topology` variable as the `topology` parameter to get any created keystore or truststore files attached as file dependencies.\n<div class=\"alert alert-block alert-info\">\n    <b>Info:</b>\nWe intentionally do not re-use the consumer properties for the producer application. In case we use certificates in any way, we have created consumer configurations that include a keystore or truststore file, which was added as a file dependency to the consumer topology. When we re-used the consumer properties here we would miss the file dependency in the prodcuer topology.\n</div>\n"}, {"metadata": {}, "cell_type": "code", "source": "producer_configs = kafka.create_connection_properties(\n    bootstrap_servers=bootstrap_servers,\n    use_TLS=connect_tls,      \n    enable_hostname_verification=True,\n    cluster_ca_cert=ca_server_cert,\n    authentication=auth,\n    client_cert=client_cert,\n    client_private_key=client_priv_key,\n    username=username,\n    password=password,\n    topology=producer_topology)\n\n# print the producer configs for reference. Note, that they can contain sensitive data\nprint()\nfor key, value in producer_configs.items():\n    print(key + \"=\" + value)\nprint()\n\nkafkaSink = kafka.publish(\n    sensorStream,\n    topic=topic,\n    kafka_properties=producer_configs,\n    name=\"SensorPublish\")", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "\nbootstrap.servers=ka.libthomas.org:9092\n\nproperties file /tmp/wsuser/producer-SensorPublish.test.properties generated.\nProperties file etc/producer-SensorPublish.test.properties added to the topology KafkaBasicSampleProducer\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"launch\"></a>\n# 4. Submit both applications to the Streams instance\nA running Streams application is called a *job*. By submitting the topologies we create two independent jobs."}, {"metadata": {}, "cell_type": "code", "source": "# disable SSL certificate verification if necessary\ncfg[context.ConfigParams.SSL_VERIFY] = False\n\n# submit consumer topology as a Streams job\nconsumer_submission_result = submit(ContextTypes.DISTRIBUTED, consumer_topology, cfg)\nconsumer_job = consumer_submission_result.job\nif consumer_job:\n    print(\"JobId of consumer job: \", consumer_job.id , \"\\nJob name: \", consumer_job.name)", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9fe8c5fa6a954054baeee8cc48b39567"}}, "metadata": {}}, {"output_type": "stream", "text": "Insecure host connections enabled.\nInsecure host connections enabled.\nInsecure host connections enabled.\n", "name": "stderr"}, {"output_type": "stream", "text": "JobId of consumer job:  70 \nJob name:  StreamsTutorialandTestbed::KafkaBasicSampleConsumer_70\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# submit producer topology as a Streams job\nproducer_submission_result = submit(ContextTypes.DISTRIBUTED, producer_topology, cfg)\nproducer_job = producer_submission_result.job\nif producer_job:\n    print(\"JobId of producer job: \", producer_job.id , \"\\nJob name: \", producer_job.name)", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "eb6a90b43a2e42ea94eb7cd0f15e39ed"}}, "metadata": {}}, {"output_type": "stream", "text": "Insecure host connections enabled.\nInsecure host connections enabled.\nInsecure host connections enabled.\n", "name": "stderr"}, {"output_type": "stream", "text": "JobId of producer job:  71 \nJob name:  StreamsTutorialandTestbed::KafkaBasicSampleProducer_71\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"view\"></a>\n# 5. Use a `View` to access data from the job\nNow that the job is started, use the `View` object you created in [step 2.3](#create_view) to start retrieving data from a `Stream`."}, {"metadata": {}, "cell_type": "code", "source": "# connect to the view and display 20 samples of the data\nqueue = streamView.start_data_fetch()\ntry:\n    for val in range(20):\n        print(queue.get())    \nfinally:\n    streamView.stop_data_fetch()", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "{'message': '{\"sensor_id\": \"sensor_60\", \"value\": 1653.961936332758, \"ts\": 1584427622}', 'key': 'sensor_60', 'topic': 'test', 'partition': 0, 'offset': 500033, 'messageTimestamp': 1584427622614}\n{'message': '{\"sensor_id\": \"sensor_60\", \"value\": 1653.961936332758, \"ts\": 1584427622}', 'key': 'sensor_60', 'topic': 'test', 'partition': 0, 'offset': 500033, 'messageTimestamp': 1584427622614}\n{'message': '{\"sensor_id\": \"sensor_75\", \"value\": 297.1178545407095, \"ts\": 1584427622}', 'key': 'sensor_75', 'topic': 'test', 'partition': 0, 'offset': 500034, 'messageTimestamp': 1584427622622}\n{'message': '{\"sensor_id\": \"sensor_84\", \"value\": 1626.972255981208, \"ts\": 1584427622}', 'key': 'sensor_84', 'topic': 'test', 'partition': 0, 'offset': 500035, 'messageTimestamp': 1584427622623}\n{'message': '{\"sensor_id\": \"sensor_86\", \"value\": 2476.2222314635846, \"ts\": 1584427622}', 'key': 'sensor_86', 'topic': 'test', 'partition': 0, 'offset': 500036, 'messageTimestamp': 1584427622625}\n{'message': '{\"sensor_id\": \"sensor_77\", \"value\": 452.5279730750628, \"ts\": 1584427622}', 'key': 'sensor_77', 'topic': 'test', 'partition': 0, 'offset': 500037, 'messageTimestamp': 1584427622626}\n{'message': '{\"sensor_id\": \"sensor_53\", \"value\": 2356.9511617757125, \"ts\": 1584427622}', 'key': 'sensor_53', 'topic': 'test', 'partition': 0, 'offset': 500038, 'messageTimestamp': 1584427622627}\n{'message': '{\"sensor_id\": \"sensor_14\", \"value\": 1944.3288797268299, \"ts\": 1584427622}', 'key': 'sensor_14', 'topic': 'test', 'partition': 0, 'offset': 500039, 'messageTimestamp': 1584427622629}\n{'message': '{\"sensor_id\": \"sensor_86\", \"value\": 1645.7663673658665, \"ts\": 1584427622}', 'key': 'sensor_86', 'topic': 'test', 'partition': 0, 'offset': 500040, 'messageTimestamp': 1584427622630}\n{'message': '{\"sensor_id\": \"sensor_41\", \"value\": 412.6775522608224, \"ts\": 1584427622}', 'key': 'sensor_41', 'topic': 'test', 'partition': 0, 'offset': 500041, 'messageTimestamp': 1584427622631}\n{'message': '{\"sensor_id\": \"sensor_87\", \"value\": 976.2961536158912, \"ts\": 1584427622}', 'key': 'sensor_87', 'topic': 'test', 'partition': 0, 'offset': 500042, 'messageTimestamp': 1584427622632}\n{'message': '{\"sensor_id\": \"sensor_32\", \"value\": 940.3783958696694, \"ts\": 1584427622}', 'key': 'sensor_32', 'topic': 'test', 'partition': 0, 'offset': 500043, 'messageTimestamp': 1584427622633}\n{'message': '{\"sensor_id\": \"sensor_85\", \"value\": 2596.8885613003727, \"ts\": 1584427622}', 'key': 'sensor_85', 'topic': 'test', 'partition': 0, 'offset': 500044, 'messageTimestamp': 1584427622635}\n{'message': '{\"sensor_id\": \"sensor_75\", \"value\": 297.1178545407095, \"ts\": 1584427622}', 'key': 'sensor_75', 'topic': 'test', 'partition': 0, 'offset': 500034, 'messageTimestamp': 1584427622622}\n{'message': '{\"sensor_id\": \"sensor_77\", \"value\": 1291.220253775594, \"ts\": 1584427622}', 'key': 'sensor_77', 'topic': 'test', 'partition': 0, 'offset': 500045, 'messageTimestamp': 1584427622636}\n{'message': '{\"sensor_id\": \"sensor_84\", \"value\": 1626.972255981208, \"ts\": 1584427622}', 'key': 'sensor_84', 'topic': 'test', 'partition': 0, 'offset': 500035, 'messageTimestamp': 1584427622623}\n{'message': '{\"sensor_id\": \"sensor_36\", \"value\": 2126.2722487606993, \"ts\": 1584427622}', 'key': 'sensor_36', 'topic': 'test', 'partition': 0, 'offset': 500046, 'messageTimestamp': 1584427622637}\n{'message': '{\"sensor_id\": \"sensor_86\", \"value\": 2476.2222314635846, \"ts\": 1584427622}', 'key': 'sensor_86', 'topic': 'test', 'partition': 0, 'offset': 500036, 'messageTimestamp': 1584427622625}\n{'message': '{\"sensor_id\": \"sensor_97\", \"value\": 2222.307077224135, \"ts\": 1584427622}', 'key': 'sensor_97', 'topic': 'test', 'partition': 0, 'offset': 500047, 'messageTimestamp': 1584427622638}\n{'message': '{\"sensor_id\": \"sensor_84\", \"value\": 976.5828946180042, \"ts\": 1584427622}', 'key': 'sensor_84', 'topic': 'test', 'partition': 0, 'offset': 500048, 'messageTimestamp': 1584427622640}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 5.1 Display the results in real time\nCalling `View.display()` from the notebook displays the results of the view in a table that is updated in real-time."}, {"metadata": {}, "cell_type": "code", "source": "# display the results for 60 seconds\nstreamView.display(duration=60)", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox(children=(HBox(children=(Text(value='received sensor data', description='ReceivedSensorData', disabled=Tr\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "61b86b9da1a34cdf9d020ea1aee5f782"}}, "metadata": {}}, {"output_type": "stream", "text": "/user-home/_global_/python-3/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "\n## 4.2 See job status \n\nYou can view job status and logs by going to **My Instances** > **Jobs**. Find your job based on the id printed above.\nRetrieve job logs using the \"Download logs\" action from the job's context menu.\n\nTo view other information about the job such as detailed metrics, access the graph. Go to **My Instances** > **Jobs**. Select \"View graph\" action for the running job.\n\n<a name=\"cancel\"></a>\n\n# 6. Cancel the jobs\n\nThis cell generates widgets you can use to cancel the jobs."}, {"metadata": {}, "cell_type": "code", "source": "# cancel the jobs in the IBM Streams service interactively\nproducer_submission_result.cancel_job_button()\nconsumer_submission_result.cancel_job_button()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can also interact with the job through the [Job](https://streamsxtopology.readthedocs.io/en/stable/streamsx.rest_primitives.html#streamsx.rest_primitives.Job) object returned from `producer_submission_result.job` and `consumer_submission_result.job`\n\nFor example, use `producer_job.cancel()` to cancel the running producer job directly."}, {"metadata": {}, "cell_type": "code", "source": "# cancel the jobs directly using the Job objects\n\n#producer_job.cancel()\n#consumer_job.cancel()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 7. Congratulation\n\nYou created a Streams application that connected to a Kafka cluster with a consumer group for load sharing and managed to connect to a Kafka cluster with potentially non-trivial connection settings. Finally you sampled the received data by using a view, and published the data within the Streams instance, so that other Streams applications in the instance can subscribe Streams internally to it.\n\nNot to forget, to bring the application to life, you also created a simple producer application, which published artificial sensor data to the Kafka topic."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}