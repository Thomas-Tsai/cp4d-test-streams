{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# IBM Streams sample application\n\nThis sample demonstrates creating a Streams Python application to perform some analytics, and viewing the results.\n\nFamiliarity with [Python](https://www.python.org/about/gettingstarted/) is recommended.\n\n\nIn this notebook, you'll see examples of how to :\n 1. [Setup a connection to the Streams instance](#setup)\n 2. [Create the application](#create)\n 3. [Submit the application](#launch)\n 4. [Connect to the running application to view data](#view)\n 5. [Stop the application](#cancel)\n\n# Overview\n\n**About the sample**\n\nThis application simulates a data hub that receives readings from sensors. It computes the 30 second rolling average of the reported readings using [Pandas](https://pandas.pydata.org/).  \n\n**How it works**\n   \nA Streams Python application processes a continuous and potentially infinite stream of data. The data is processed in memory and is not stored in a database first.\n\nThe Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to continuously retrieve the results.\n\n<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n\n\n### Documentation\n\n- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n\n\n\n\n# Prerequisites\n\nThis notebook can be used as-is from within an IBM Cloud Pak for Data project. \n\nIf you are not running this notebook from within IBM Cloud Pak for Data, [follow these steps to make sure you have installed all the prerequisites](https://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-2/).\n\n<a name=\"setup\"></a>\n\n# 1. Set up a connection to the Streams instance\n\n\nTo submit the application for execution, you have to connect to the Streams instance. The application is always the same, but the information required to connect to the instance depends on the target installation of Streams.\n\n- [I'm running the notebook from an IBM Cloud for Data project](#cpd)\n- [I'm using IBM Watson Studio, Jupyter Notebooks, or any other development environment](#notcpd)\n\n\n\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"cpd\"></a>\n### 1.1a Connect to a Streams instance from an IBM Cloud Pak for Data  project\n\nIn order to submit a Streams application you need to provide the name of the Streams instance.\n\n1. From the navigation menu, click **My instances**.\n2. Click the **Provisioned Instances** tab.\n3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name\n4. Run the cell and skip to section 1.2\n\nThe cell below defines a function called `submit_topology` that will be used later on to submit the `Topology` once it is defined.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "from icpd_core import icpd_util\nfrom streamsx.topology import context\n\nstreams_instance_name = \"cp4d-streams-instance\" ## Change this to Streams instance\ncfg=icpd_util.get_service_instance_details(name=streams_instance_name)\n\ndef submit_topology(topo):\n    global cfg\n    # Disable SSL certificate verification if necessary\n    cfg[context.ConfigParams.SSL_VERIFY] = False\n    # Topology wil be deployed as a distributed app\n    contextType = context.ContextTypes.DISTRIBUTED\n    return context.submit (contextType, topo, config = cfg)", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"notcpd\"></a>\n### 1.1b Connect to a Streams instance from IBM Watson Studio and other environments\n\nThe code for each scenario is available in the development guide.\n\n- Choose the tab that best matches your environment. Copy the code under the heading **Copy this code snippet**.\n- Paste it in the cell below.\n\n[Connection instructions from the development guide](https://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-3/#1-set-up-a-connection-to-the-streams-instance)\n\nEach snippet will define a function called `submit_topology` that will be used later on to submit the `Topology` once it is defined."}, {"metadata": {}, "cell_type": "code", "source": "## Paste snippet from development guide\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Verify `streamsx` package version\n\nRun the cell below to check which version of the `streamsx` package is installed.  \n\nIf you need to upgrade, use\n\n- `import sys`\n- `!{sys.executable} -m pip install --user --upgrade streamsx` to upgrade the package.\n- Or, use  `!{sys.executable} -m pip install --user streamsx==somever` to install a specific version of the package. \n"}, {"metadata": {}, "cell_type": "code", "source": "import streamsx.topology.context\nprint(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\n\n#For more details uncomment line below.\n#!pip show streamsx", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "INFO: streamsx package version: 1.13.14\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"create\"></a>\n# 2. Create the application\n \n\nAll Streams applications start with  a `Topology` object, so start by creating one:\n"}, {"metadata": {}, "cell_type": "code", "source": "from streamsx.topology.topology import Topology\n\ntopo = Topology(name=\"SensorAverages\")", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.1 Define sources\nYour application needs some data to analyze, so the first step is to define a data source that produces the data to be analyzed. \n\nNext, use the data source to create a `Stream` object. A `Stream` is a potentially infinite sequence of tuples containing the data to be analyzed.\n\nTuples are Python objects by default. Other supported formats include JSON. [See the doc for all supported formats](https://streamsxtopology.readthedocs.io/en/stable/streamsx.topology.topology.html#stream)."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.1 Define a source class\n\nDefine a callable class that will produce the data to be analyzed.\n\nThis example class simulates readings from sensors. Each reading is a Python `dict` containing the sensor id, the reported value, and the timestamp of the reading."}, {"metadata": {}, "cell_type": "code", "source": "import random \nimport time\nfrom datetime import datetime, timedelta\n\n# Define a callable source \nclass SensorReadingsSource(object):\n    def __call__(self):\n        # This is just an example of using generated data, \n        # Here you could connect to db\n        # generate data\n        # connect to data set\n        # open file\n        while True:\n            time.sleep(0.001)\n            sensor_id = random.randint(1,100)\n            reading = {}\n            reading [\"sensor_id\"] = \"sensor_\" + str(sensor_id)\n            reading [\"value\"] =  random.random() * 3000\n            reading[\"ts\"] = int((datetime.now().timestamp())) \n            yield reading ", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2  Create the `Stream `\n\nCreate a `Stream` called  `readings` that will contain the simulated data that `SensorReadingsSource` produces:"}, {"metadata": {}, "cell_type": "code", "source": "#Create a stream from the data using Topology.source\nreadings = topo.source(SensorReadingsSource(), name=\"Readings\")\n", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2.2 Analyze data\n\nUse a variety of methods in the `Stream` class to analyze your in-flight data, including applying machine learning models.\n \nThis section will:\n- Filter out tuples based on a condition,\n- Compute the rolling average, and\n- Enrich the rolling average with data from another source.\n\nBuilt-in methods exist for common operations, such as <code>Stream.filter</code> and <code>Stream.split</code>, which filter or split a stream of data respectively.\n\nSee the <a href=\"/streamsx.documentation/docs/python/1.6/python-appapi-devguide-4/\"> common operations section</a> for other common examples. Check out the <a href=\"https://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#streamsx.topology.topology.Stream\">documentation </a> of the <code>Stream</code> class for full list of functions."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2.1 Filter data from the  `Stream`  \n\nUse `Stream.filter()` to remove data that doesn't match a certain condition."}, {"metadata": {}, "cell_type": "code", "source": "# Accept only values greater than 100\n\nvalid_readings = readings.filter(lambda x : x[\"value\"] > 100,\n                                 name=\"ValidReadings\")\n\n# You could create another stream of the invalid data:\n# invalid_readings = readings.filter(lambda x : x[\"value\"] <= 100,)", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2.2  Compute averages on the  `Stream`  \n\nDefine a function to compute the 30 second rolling average for the readings.\n\nSteps are outlined in the code below.\nSee the [Window class documentation](https://streamsxtopology.readthedocs.io/en/stable/streamsx.topology.topology.html#streamsx.topology.topology.Window)  for details.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\n\n# 1. Define aggregation function\n    \ndef average_reading(items_in_window):\n    df = pd.DataFrame(items_in_window)\n    readings_by_id = df.groupby(\"sensor_id\")\n    \n    averages = readings_by_id[\"value\"].mean()\n    period_end = df[\"ts\"].max()\n\n    result = []\n    for id, avg in averages.iteritems():\n        result.append({\"average\": avg,\n                \"sensor_id\": id,\n                \"period_end\": time.ctime(period_end)})\n               \n    return result\n\n# 2. Define window: e.g. a 30 second rolling average, updated every second\n\ninterval = timedelta(seconds=30)\nwindow = valid_readings.last(size=interval).trigger(when=timedelta(seconds=1))\n\n\n# 3. Pass aggregation function to Window.aggregate\n# average_reading returns a list of the averages for each sensor,\n# use flat map to convert it to individual tuples, one per sensor\nrolling_average = window.aggregate(average_reading).flat_map()\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n### 2.2.3 Enrich the data on the `Stream`\n\nEach tuple on the `rolling_average Stream` will have the following format: \n\n`{'average': 1655.1009278955999, 'sensor_id': 'sensor_17', 'period_end': 'Tue Nov 19 22:07:02 2019'}\n`. (See the `average_reading` function above).\n\nImagine that you want to add the geographical coordinates of the sensor to each tuple. This information might come from a different data source, such as a database.\n\nUse `Stream.map()`. The `map` transfrom uses a function you provide to convert each tuple on the `Stream` into a new tuple. \n\nIn our case, for each tuple on the `rolling_average Stream`,  we will update it to include the geographical coordinates of the sensor.\n"}, {"metadata": {}, "cell_type": "code", "source": "# Modify this tuple with the coordinates of the sensor\n# Returns the original tuple with a new `coords` attribute\n# representing the latitude and longitude of the sensor\ndef enrich(tpl):\n    # use simulated data, but you could make a database call, \n    lat = round(random.random() + 39.8338515, 4)\n    lon = round(-74.871826 + random.random(), 4)\n    # update the tuple with new data\n    tpl[\"coords\"] = (lat, lon)\n    return tpl\n\n# Update the data on the rolling_average stream with the map transform\nenriched_average = rolling_average.map(enrich)\n", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2.3 Create a `View` to preview the tuples on the `Stream` \n\n\nA `View` is a connection to a `Stream` that becomes activated when the application is running. The connection allows you to access the data on the `Stream` as it is being processed.\n\n\nAfter submitting the `Topology`, we use a `View`  to examine the from within the notebook [in section 4](#view).\n\nTo view the data on the `enriched_average Stream`, define a `View` using `Stream.view()`:\n"}, {"metadata": {}, "cell_type": "code", "source": "averages_view = enriched_average.view(name=\"RollingAverage\", \n                                      description=\"Sample of rolling averages for each sensor\")", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can <a href=\"http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-6/#accessing-the-tuples-of-a-view\">connect to a view in <em>any</em> running Streams job using the REST API</a> , regardless of what language was used to create the application.\n\n# 2.4 Define output\n\nYou could also enable a microservices based architecture by publishing the results so that other Streams applications can connect to it.\n\nUse `Stream.publish()` to make the `enriched_average Stream` available to other applications. \n\nTo send the stream to another database or system, use a sink function (similar to the source function) and invoke it using `Stream.for_each`.\n"}, {"metadata": {}, "cell_type": "code", "source": "import json\n# publish results as JSON\nenriched_average.publish(topic=\"AverageReadings\",\n                        schema=json, \n                        name=\"PublishAverage\")\n\n# Other options include:\n# invoke another sink function:\n# rolling_average.for_each(func=send_to_db)", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "<streamsx.topology.topology.Sink at 0x7fdb48075080>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"launch\"></a>\n\n# 3. Submit the application\nA running Streams application is called a *job*. Use this cell to submit the `Topology` for execution, using the `submit_topology` function [defined in step 1](#setup)."}, {"metadata": {}, "cell_type": "code", "source": "# The submission_result object contains information about the running application, or job\nprint(\"Submitting Topology to Streams for execution..\")\nsubmission_result = submit_topology(topo)\n\nif submission_result.job:\n  streams_job = submission_result.job\n  print (\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)\nelse:\n  print(\"Submission failed: \"   + str(submission_result))", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Submitting Topology to Streams for execution..\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b2a385bb3ee9476f91503b0a412202df"}}, "metadata": {}}, {"output_type": "stream", "text": "Insecure host connections enabled.\nInsecure host connections enabled.\nInsecure host connections enabled.\n/user-home/_global_/python-3/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n", "name": "stderr"}, {"output_type": "stream", "text": "JobId:  57 \nJob name:  StreamsTutorialandTestbed::SensorAverages_57\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"view\"></a>\n\n# 4. Use a `View` to access data from the job\n\nNow that the job is started, use the `averages_view` object you created in step 2.3 to start retrieving data from the `enriched_average Stream`."}, {"metadata": {}, "cell_type": "code", "source": "print(\"Fetching view data ...\")\n# Connect to the view and display the data\nqueue = averages_view.start_data_fetch()\ntry:\n    for val in range(10):\n        print(queue.get())    \nfinally:\n    averages_view.stop_data_fetch()", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Fetching view data ...\n{'average': 1536.2641721286375, 'sensor_id': 'sensor_100', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.1957, -74.2939]}\n{'average': 1669.0316114865639, 'sensor_id': 'sensor_11', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.0728, -74.5729]}\n{'average': 1500.6779491661089, 'sensor_id': 'sensor_12', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.2664, -73.9385]}\n{'average': 1608.8873735653708, 'sensor_id': 'sensor_13', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [39.9446, -74.2108]}\n{'average': 1537.161351949015, 'sensor_id': 'sensor_14', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.3, -73.9264]}\n{'average': 1543.8344748146496, 'sensor_id': 'sensor_15', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.1165, -74.7254]}\n{'average': 1647.3034116054428, 'sensor_id': 'sensor_16', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [39.9809, -74.1446]}\n{'average': 1557.3639487670048, 'sensor_id': 'sensor_17', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.1277, -74.8606]}\n{'average': 1621.3170070764832, 'sensor_id': 'sensor_18', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.5481, -74.6515]}\n{'average': 1627.1380503952648, 'sensor_id': 'sensor_19', 'period_end': 'Tue Mar 10 06:13:50 2020', 'coords': [40.7989, -74.2772]}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.1 Display the results in real time\nCalling `View.display()` from the notebook displays the results of the view in a table that is updated in real-time."}, {"metadata": {}, "cell_type": "code", "source": "# Display the results for 30 seconds\naverages_view.display(duration=30)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox(children=(HBox(children=(Text(value='Sample of rolling averages for each sensor', description='RollingAve\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8e7458c0751e4837a8d637fe796b9dde"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "\n## 4.2 See job status \n\nIn IBM Cloud Pak for Data, you can view job status and logs with the Job Graph.\n\nTo view job status and logs:\n<ol>\n<li>From the main menu, go to <b>My Instances &gt; Jobs</b>. </li>\n<li>Find your job based on the <code>JobId</code> printed when you submitted the topology.</li>\n<li>Select <b>View graph</b> from the context menu action for the running job.</li>\n</ol>\n\nFor all other development environments, use the Streams Console.\n\n[See instructions and an example](http://ibm.biz/Bdz6yD)."}, {"metadata": {}, "cell_type": "markdown", "source": "<a name=\"cancel\"></a>\n\n# 5. Cancel the job\nStreams jobs run indefinitely, so make sure you cancel the job once you are finished running the sample.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "This cell generates a widget you can use to cancel the job."}, {"metadata": {}, "cell_type": "code", "source": "#cancel the job in the IBM Streams service\nsubmission_result.cancel_job_button()", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox(children=(Button(button_style='danger', description='Cancel job: StreamsTutorialandTestbed::SensorAverage\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "431c0f0e2a154a84a8a1a3edb64b1521"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "You can also interact with the job through the [Job](https://streamsxtopology.readthedocs.io/en/stable/streamsx.rest_primitives.html#streamsx.rest_primitives.Job) object returned from `submission_result.job`\n\nFor example, use `job.cancel()` to cancel the running job directly."}, {"metadata": {}, "cell_type": "markdown", "source": "# Summary\n\nWe started with a `Stream` called `readings`, which contained the data we wanted to analyze. Next, we used functions in the `Stream` object to perform simple analysis and produced the `enriched_average` stream.  This stream is `published` for other applications running within our Streams instance to access.\n\nAfter submitting the application to the Streams service, we used the `enriched_average` view to see the results.\n\n\n\n# Learn more \n\n- **Find more samples**: This notebook is one of several sample notebooks available in the [starter notebooks repository on GitHub](https://github.com/IBMStreams/sample.starter_notebooks). Visit the repository for examples of how to connect to common data sources, including Apache Kafka, IBM, and Db2 Warehouse. \n\n\n- Learn more about how to use the API from the [development guide](http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide/)."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}